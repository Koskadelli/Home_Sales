# Home_Sales
Module 22 Challenge

# Premise
In this challenge, I use SparkSQL (PySpark with SQL queries) to determine key metrics about home sales data. I also performed efficiency tests via calculating runtime while processing the data (testing create temporary views, partitioning the data, and cache and uncache a temporary table). 

# Resources
* StackOverflow thread assisting with hadoop errors when writing parquets: https://stackoverflow.com/questions/41851066/exception-in-thread-main-java-lang-unsatisfiedlinkerror-org-apache-hadoop-io
* PySpark SQL documentation: https://spark.apache.org/docs/3.1.2/api/python/reference/pyspark.sql.html
* ChatGPT for select coding assistance